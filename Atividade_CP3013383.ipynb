{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f414f8",
   "metadata": {},
   "source": [
    "Atividade apresentada a disciplina de Tópicos do Curso de Espoecialização em Ciência de Dados.\n",
    "\n",
    "Giovanna Nascimento Antonieti          CP3013383\n",
    "\n",
    "O conjunto de dados está disponível através do link https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b6e9a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85c895a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giovannaantonieti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install numpy pandas scikit-learn tensorflow nltk gensim\n",
    "#!conda install numpy pandas scikit-learn tensorflow nltk gensim -y\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import eli5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e2f02",
   "metadata": {},
   "source": [
    "# Definição das variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "batch_size=128\n",
    "max_words=100000\n",
    "max_seq_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56375abb",
   "metadata": {},
   "source": [
    "# Funções de limpeza\n",
    "\n",
    "Funções para remover pontuação, abreviação, emojis e caracteres especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc040ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n",
    "    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "    \"Trump's\": \"trump is\", \"Obama's\": \"obama is\", \"Canada's\": \"canada is\", \"today's\": \"today is\"\n",
    "}\n",
    "    \n",
    "def clean_contractions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    words = [contraction_mapping[word] if word in contraction_mapping else word for word in text.split(\" \")]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bc3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token for token in tokens if token not in mystopwords]\n",
    "    return [\" \".join(remove_stops_digits(word_tokenize(content,preserve_line=True))) for content in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dffe174",
   "metadata": {},
   "outputs": [],
   "source": [
    "specail_signs = { \"…\": \"...\", \"₂\": \"2\"}\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def clean_special_chars(text):\n",
    "    for s in specail_signs: \n",
    "        text = text.replace(s, specail_signs[s])\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ef12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_caps_mapping = { \n",
    "    \"ᴀ\": \"a\", \"ʙ\": \"b\", \"ᴄ\": \"c\", \"ᴅ\": \"d\", \"ᴇ\": \"e\", \"ғ\": \"f\", \"ɢ\": \"g\", \"ʜ\": \"h\", \"ɪ\": \"i\", \"ᴊ\": \"j\", \"ᴋ\": \"k\", \"ʟ\": \"l\", \"ᴍ\": \"m\",\n",
    "    \"ɴ\": \"n\", \"ᴏ\": \"o\", \"ᴘ\": \"p\", \"ǫ\": \"q\", \"ʀ\": \"r\", \"s\": \"s\", \"ᴛ\": \"t\", \"ᴜ\": \"u\", \"ᴠ\": \"v\", \"ᴡ\": \"w\", \"x\": \"x\", \"ʏ\": \"y\", \"ᴢ\": \"z\",\n",
    "    \"𝘊\": \"C\", \"𝘦\": \"e\", \"𝘳\": \"r\", \"𝘢\": \"a\", \"𝘵\": \"t\", \"𝘰\": \"o\", \"𝘤\": \"c\", \"𝘺\": \"y\", \"𝘴\": \"s\", \"𝘪\": \"i\", \"𝘧\": \"f\", \"𝘮\": \"m\", \"𝘣\": \"b\",\n",
    "    \"м\": \"m\", \"υ\": \"u\", \"т\": \"t\", \"ѕ\": \"s\", \"𝙀\": \"E\", \"𝒛\": \"z\", \"𝑲\": \"K\", \"𝑳\": \"L\", \"𝑾\": \"W\", \"𝒋\": \"j\", \"𝟒\": \"4\",\n",
    "    \"𝙒\": \"W\", \"𝘾\": \"C\", \"𝘽\": \"B\", \"𝑱\": \"J\", \"𝑹\": \"R\", \"𝑫\": \"D\", \"𝑵\": \"N\", \"𝑪\": \"C\", \"𝑯\": \"H\", \"𝒒\": \"q\", \"𝑮\": \"G\", \"𝗕\": \"B\", \"𝗴\": \"g\", \n",
    "    \"𝟐\": \"2\", \"𝗸\": \"k\", \"𝗟\": \"L\", \"𝗠\": \"M\", \"𝗷\": \"j\", \"𝐎\": \"O\", \"𝐍\": \"N\", \"𝐊\": \"K\", \"𝑭\": \"F\", \"Е\": \"E\"\n",
    "}\n",
    "\n",
    "def clean_small_caps(text):\n",
    "    for char in special_caps_mapping:\n",
    "        text = text.replace(char, special_caps_mapping[char])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a2065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = \"🍕🐵😑😢🐶️😜😎👊😁😍💖💵👎😀😂🔥😄🏻💥😋👏😱🚌ᴵ͞🌟😊😳😧🙀😐😕👍😮😃😘💩💯⛽🚄😖🏼🚲😟😈💪🙏🎯🌹😇💔😡👌🙄😠😉😤⛺🙂😏🍾🎉😞🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪🐰🐇🐱🙆😨🙃💕💗💚🙈😴🏿🤗🇺🇸⤵🏆🎃😩👮💙🐾🐕😆🌠🐟💫💰💎🖐🙅⛲🍰🤐👆🙌💛🙁👀🙊🙉🚬🤓😵😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓😣⏺😌🤑🌏😯😲💞🚓🔔📚🏀👐💤🍇🏡❔⁉👠》🇹🇼🌸🌞🎲😛💋💀🎄💜🤢َِ🗑💃📣👿༼つ༽😰🤣🐝🎅🍺🎵🌎͟🤡🤥😬🤧🚀🤴😝💨🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦🍀😫🤤🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪🇫🇷🇩🇪😷🇨🇦🌐📺🐋💘💓💐🌋🌄🌅👺🐷🚶🤘ͦ💸👂👃🎫🚢🚂🏃👽😙🎾👹⎌🏒⛸🏄🐀🚑🤷🤙🐒🐈ﷻ🦄🚗🐳👇⛷👋🦊🐽🎻🎹⛓🏹🍷🦆♾🎸🤕🤒⛑🎁🏝🦁🙋😶🔫👁💲🗯👑🚿💡😦🏐🇰🇵👾🐄🎈🔨🐎🤞🐸💟🎰🌝🛳🍭👣🏉💭🎥🐴👨🤳🦍🍩😗🏂👳🍗🕉🐲🍒🐑⏰💊🌤🍊🔹🤚🍎𝑷🐂💅💢💒🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻🤖🎎😼🕷👼📉🍟🍦🌈🔭《🐊🐍🐦🐡💳ἱ🙇🥜🔼\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbe1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_process(text):\n",
    "    text = text.lower()\n",
    "    text = clean_contractions(text)\n",
    "    text = clean_special_chars(text)\n",
    "    text = clean_small_caps(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f78e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(pipe, x_test, y_test):\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    report = metrics.classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ab433",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed345874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61640d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e352fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba7b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['target','comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ad20df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                       comment_text\n",
       "0  0.000000  This is so cool. It's like, 'would you want yo...\n",
       "1  0.000000  Thank you!! This would make my life a lot less...\n",
       "2  0.000000  This is such an urgent design problem; kudos t...\n",
       "3  0.000000  Is this something I'll be able to install on m...\n",
       "4  0.893617               haha you guys are a bunch of losers."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec2f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino início:  17 : 21 : 27\n",
      "Teste início:  17 : 22 : 3\n",
      "Fim:  17 : 22 : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Treino início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda text: clean_process(text))\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Teste início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "\n",
    "test['comment_text'] = test['comment_text'].apply(lambda text: clean_process(text))\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "\n",
    "\n",
    "y = train['target']\n",
    "y = np.where(y>=0.5,1,0)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f011e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início:  17 : 25 : 27\n",
      "Fim:  17 : 29 : 1\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "X_stopwords = preprocess_corpus(train['comment_text'])\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67e6dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b780360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928bbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início:  17 : 29 : 3\n",
      "Fim:  17 : 29 : 15\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "X_test_stopwords = preprocess_corpus(test['comment_text'])\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f981549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97320,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a29fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97320,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_test_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18c955",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fc1748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início:  17 : 29 : 32\n",
      "Fim:  17 : 30 : 29\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "tranformer = Tokenizer(lower = True, filters='', num_words=max_words)\n",
    "tranformer.fit_on_texts( list(X_stopwords) + list(X_test_stopwords) )\n",
    "transformed_x = tranformer.texts_to_sequences(X_stopwords)\n",
    "transformed_x = pad_sequences(transformed_x, maxlen = max_seq_size)\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23bf08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tranformer.texts_to_sequences(X_test_stopwords)\n",
    "X_test = pad_sequences(X_test, maxlen = max_seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46fc3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início:  17 : 31 : 10\n",
      "Fim:  17 : 31 : 15\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(transformed_x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f35bbb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negitive (1) class len is : 115256\n",
      "positive (0) class len is : 1689618\n"
     ]
    }
   ],
   "source": [
    "neg_class_len = np.sum(y_train)\n",
    "pos_class_len =train.shape[0] - int(neg_class_len)\n",
    "print('negitive (1) class len is :',int(neg_class_len))\n",
    "print('positive (0) class len is :',pos_class_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "102e3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./numpy_arrays/X_train.npy\", np.asarray(X_train))\n",
    "np.save(\"./numpy_arrays/Y_train.npy\", np.asarray(y_train))\n",
    "np.save(\"./numpy_arrays/X_val.npy\", np.asarray(X_val))\n",
    "np.save(\"./numpy_arrays/Y_val.npy\", np.asarray(y_val))\n",
    "np.save(\"./numpy_arrays/X_test.npy\", np.asarray(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de90f5",
   "metadata": {},
   "source": [
    "# Modelos Clássicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591e077",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "97100815",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ace80e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início treinamento:  7 : 36 : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/topicos/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim treinamento:  7 : 40 : 44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96    331897\n",
      "           1       0.06      0.00      0.00     29078\n",
      "\n",
      "    accuracy                           0.92    360975\n",
      "   macro avg       0.49      0.50      0.48    360975\n",
      "weighted avg       0.85      0.92      0.88    360975\n",
      "\n",
      "accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início treinamento: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim treinamento: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "\n",
    "print_report(lr, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38d76",
   "metadata": {},
   "source": [
    "## Regressão SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5d0d0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4a96d58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início treinamento:  7 : 42 : 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/topicos/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim treinamento:  7 : 44 : 50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.75      0.83    331897\n",
      "           1       0.08      0.24      0.12     29078\n",
      "\n",
      "    accuracy                           0.71    360975\n",
      "   macro avg       0.50      0.50      0.47    360975\n",
      "weighted avg       0.85      0.71      0.77    360975\n",
      "\n",
      "accuracy: 0.710\n"
     ]
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "print(\"Início treinamento: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Fim treinamento: \", dateTimeObj.hour, ':', dateTimeObj.minute, ':', dateTimeObj.second)\n",
    "\n",
    "print_report(svm, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf428ad",
   "metadata": {},
   "source": [
    "# Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9cd6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_embedding(vec_files):\n",
    "    \n",
    "    # convert victor to float16 to make it use less memory\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float16')\n",
    "\n",
    "    # make our embed smaller by get_coefs\n",
    "    def optimize_embedding(embedding): \n",
    "        optimized_embedding = {}\n",
    "        for word in embedding.key_to_index.keys():\n",
    "            optimized_embedding[word] = np.asarray(embedding[word], dtype='float16')\n",
    "        return optimized_embedding\n",
    "\n",
    "    \n",
    "    # load embed vector from file\n",
    "    def load_embed(file):\n",
    "        print(\"Loading {}\".format(file))\n",
    "\n",
    "        if file == './modelo_wordembedding/crawl-300d-2M.vec':\n",
    "            return optimize_embedding(KeyedVectors.load_word2vec_format(file))\n",
    "\n",
    "        else:\n",
    "            return dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "        \n",
    "    combined_embedding = {}\n",
    "    for file in vec_files:\n",
    "        combined_embedding.update(load_embed(file))\n",
    "\n",
    "def build_embedding_matrix(word_index, total_vocab, embedding_size):\n",
    "    embedding_index = combine_embedding(vec_files)\n",
    "    matrix = np.zeros((total_vocab, embedding_size))\n",
    "    for index, word in enumerate(word_index.items()):\n",
    "        try:\n",
    "            matrix[index] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5d1097bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./modelo_wordembedding/crawl-300d-2M.vec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [177]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m total_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m----> 4\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_embedding_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [176]\u001b[0m, in \u001b[0;36mbuild_embedding_matrix\u001b[0;34m(word_index, total_vocab, embedding_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(word_index\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         matrix[index] \u001b[38;5;241m=\u001b[39m \u001b[43membedding_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "word_index = tranformer.word_index\n",
    "total_vocab = len(word_index) + 1\n",
    "embedding_size = 300\n",
    "embedding_matrix = build_embedding_matrix(tranformer.word_index, total_vocab, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(max_seq_size,), dtype='int32')\n",
    "embedding_layer = tensorflow.keras.layers.Embedding(total_vocab,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq_size,\n",
    "                            trainable=False)\n",
    "\n",
    "x_layer = embedding_layer(sequence_input)\n",
    "x_layer = tensorflow.keras.layers.SpatialDropout1D(0.2)(x_layer)\n",
    "x_layer = tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(64, return_sequences=True))(x_layer)   \n",
    "x_layer = tensorflow.keras.layers.Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_layer)\n",
    "\n",
    "avg_pool1 = tensorflow.keras.layers.GlobalAveragePooling1D()(x_layer)\n",
    "max_pool1 = tensorflow.keras.layers.GlobalMaxPooling1D()(x_layer)     \n",
    "\n",
    "x_layer = tensorflow.keras.layers.concatenate([avg_pool1, max_pool1])\n",
    "\n",
    "preds = tensorflow.keras.layers.Dense(1, activation='sigmoid')(x_layer)\n",
    "\n",
    "model = tensorflow.keras.Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7df757",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059669d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d47517",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install matplotlib -y\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "ax1.set_xticks(np.arange(1, epochs, 1))\n",
    "plt.legend(loc='best', shadow=True)\n",
    "\n",
    "ax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "ax2.set_xticks(np.arange(1, epochs, 1))\n",
    "plt.legend(loc='best', shadow=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57218faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(model, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
